{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d64cd5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Gridap day at \"Groupe Calcul\"\n",
    "\n",
    "# Exercise 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d291f66",
   "metadata": {},
   "source": [
    "The following parallel code computes the Poisson equation with Dirichlet boundary conditions on the unit cube. It measures the time of the main phases of the algorithm and saves the results to a file. `n` is the number of cells in each direction. `np` is the number of parts in each direction. `nruns` is the number of times the algorithm is run. We need to run several times to obtain reliable time measures.\n",
    "\n",
    "The exercise consists in running this example in parallel using MPI on a system of your choice. In a laptop you should be able to run for a problem of size of `n=(50,50,50)` for `np=(1,1,1)`, `np=(2,1,1)` and `np=(2,2,1)` and see some speedup as the number of MPI rank increases. In a cluster, you should be able tu run for much larger problem sizes and more MPI ranks.\n",
    "\n",
    "Main steps.\n",
    "\n",
    "1. System Image. Generate a package e.g. with `pkg> generate MyPackage` and copy the main function below into the package. Generate a system image of this package using PackageCompiler.jl. The cell starting with `# compile.jl` contains the code needed to generate the system image. Copy the contents of the cell starting with `# warmup.jl` into a file called `warmup.jl` and use it as a warm-up script. This will take several minutes to finish.\n",
    "\n",
    "3. Run the code in parallel with a command like `mpiexec -np 4 julia --project=. -J MySysImg.so driver.jl`. Don't forget to use the system image. The version of `mpiexec` needs to match with the installation being used by MPI.jl. See MPI.jl (v0.19) for details. The contents of file `driver.jl` are given in a cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170750f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gridap\n",
    "using GridapDistributed\n",
    "using GridapPETSc\n",
    "using PartitionedArrays\n",
    "using FileIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "function main(parts,cells,i,options)\n",
    "    timer = PTimer(parts)\n",
    "    u(x) = sum(x)\n",
    "    f(x) = -Δ(u,x)\n",
    "    g = u\n",
    "    k = 1\n",
    "    domain = (0,1,0,1,0,1)\n",
    "    tic!(timer,barrier=true)\n",
    "    model = CartesianDiscreteModel(parts,domain,cells)\n",
    "    toc!(timer,\"mesh\")\n",
    "    tic!(timer,barrier=true)\n",
    "    reffe = ReferenceFE(lagrangian,Float64,k)\n",
    "    Vh = TestFESpace(model,reffe,dirichlet_tags=\"boundary\")\n",
    "    Uh = TrialFESpace(Vh,g)\n",
    "    toc!(timer,\"space\")\n",
    "    tic!(timer,barrier=true)\n",
    "    Ω = Interior(model)\n",
    "    dΩ = Measure(Ω,2*k)\n",
    "    a(u,v) = ∫(∇(u)⋅∇(v))dΩ\n",
    "    l(v) = ∫(v*f)dΩ\n",
    "    op = AffineFEOperator(a,l,Uh,Vh)\n",
    "    toc!(timer,\"assembly\")\n",
    "    uh = GridapPETSc.with(args=split(options)) do\n",
    "        solver = PETScLinearSolver()\n",
    "        tic!(timer,barrier=true)\n",
    "        uh = solve(solver,op)\n",
    "        toc!(timer,\"solver\")\n",
    "        uh\n",
    "    end\n",
    "    tic!(timer,barrier=true)\n",
    "    e = u - uh\n",
    "    contrib_h1 = ∫(∇(e)⋅∇(e) + e*e )dΩ\n",
    "    h1 = sqrt(sum(contrib_h1))\n",
    "    toc!(timer,\"error\")\n",
    "    display(timer)\n",
    "    map_main(timer.data) do data\n",
    "        ns = join(map(c->\"_\"*string(c),cells))\n",
    "        nps = join(map(c->\"_\"*string(c),size(parts)))\n",
    "        name = \"results_run$(i)$(ns)$(nps).jld2\"\n",
    "        output = Dict{String,Any}(data)\n",
    "        output[\"h1\"] = h1\n",
    "        save(name,output)\n",
    "    end\n",
    "    nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33911a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile.jl\n",
    "using PackageCompiler\n",
    "create_sysimage([:Example1],\n",
    "  sysimage_path=joinpath(@__DIR__,\"MyPackage.so\"),\n",
    "  precompile_execution_file=joinpath(@__DIR__,\"warmup.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e85191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warmup.jl\n",
    "using PartitionedArrays\n",
    "using MyPackage\n",
    "backend = MPIBackend()\n",
    "n = (10,10,10)\n",
    "np = (1,1,1)\n",
    "options = \"-pc_type gamg -ksp_type cg -ksp_converged_reason -ksp_rtol 1.0e-12\"\n",
    "MyPackage.main(parts,n,1,options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.jl\n",
    "using PartitionedArrays\n",
    "using MyPackage\n",
    "backend = MPIBackend()\n",
    "n = (50,50,50)\n",
    "np = (2,2,1)\n",
    "nruns = 3\n",
    "options = \"-pc_type gamg -ksp_type cg -ksp_converged_reason -ksp_rtol 1.0e-12\"\n",
    "prun(backend,np) do parts\n",
    "    for i in 1:nruns\n",
    "        MyPackage.main(parts,n,i,options)\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
